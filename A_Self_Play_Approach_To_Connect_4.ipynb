{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP2_y6QUda1s"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from numpy.typing import NDArray\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import json\n",
        "import time\n",
        "from typing import List, Tuple, Deque, Optional, Callable\n",
        "import gym_connect4\n",
        "import gym\n",
        "import torch\n",
        "import collections\n",
        "import torch.nn.init as init\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.optim.lr_scheduler import _LRScheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "metadata": {
        "id": "YLruknGoYjzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Connect4-v0', height=6, width=9, connect=4)"
      ],
      "metadata": {
        "id": "qy94SdUNYk7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectiveFunction:\n",
        "\n",
        "    def __init__(self, env, policy,best_policy, num_episodes=1, max_time_steps=float('inf'), minimization_solver=True):\n",
        "        self.ndim = policy.num_params\n",
        "        self.env = env\n",
        "        self.policy = policy\n",
        "        self.best_policy = best_policy\n",
        "        self.num_episodes = num_episodes\n",
        "        self.max_time_steps = max_time_steps\n",
        "        self.minimization_solver = minimization_solver\n",
        "\n",
        "        self.num_evals = 0\n",
        "\n",
        "    def eval(self, policy_params, num_episodes=None, max_time_steps=None, render=False,win_threshold=0.7):\n",
        "        self.num_evals += 1\n",
        "        if num_episodes is None:\n",
        "            num_episodes = self.num_episodes\n",
        "\n",
        "        if max_time_steps is None:\n",
        "            max_time_steps = self.max_time_steps\n",
        "        average_total_rewards = 0\n",
        "        number_of_win = 0\n",
        "        return average_total_rewards\n",
        "\n",
        "    def __call__(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
        "        return self.eval(policy_params, num_episodes, max_time_steps, render)"
      ],
      "metadata": {
        "id": "wnc5X8CKv0c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(torch.nn.Module):\n",
        "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int, nn_l3=0, nn_l4=0, nn_l5 =0):\n",
        "\n",
        "        if(nn_l3 == 0):\n",
        "            nn_l3 = nn_l2\n",
        "        if(nn_l4 == 0):\n",
        "           nn_l4 = nn_l3\n",
        "        if(nn_l5 == 0):\n",
        "           nn_l5 = nn_l4\n",
        "\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        self.layer1 = torch.nn.Linear(n_observations, nn_l1)\n",
        "        self.layer2 = torch.nn.Linear(nn_l1, nn_l2)\n",
        "        self.layer3 = torch.nn.Linear(nn_l2, nn_l3)\n",
        "        self.layer4 = torch.nn.Linear(nn_l3, nn_l4)\n",
        "        self.layer5 = torch.nn.Linear(nn_l4, nn_l5)\n",
        "        self.layer6 = torch.nn.Linear(nn_l5, n_actions)\n",
        "        self.leaky_relu = torch.nn.LeakyReLU()\n",
        "\n",
        "        init.xavier_uniform_(self.layer1.weight)\n",
        "        init.xavier_uniform_(self.layer2.weight)\n",
        "        init.xavier_uniform_(self.layer3.weight)\n",
        "        init.xavier_uniform_(self.layer4.weight)\n",
        "        init.xavier_uniform_(self.layer5.weight)\n",
        "        init.xavier_uniform_(self.layer6.weight)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.to(self.layer1.weight.dtype)\n",
        "        x = self.leaky_relu(self.layer1(x))\n",
        "        x = self.leaky_relu(self.layer2(x))\n",
        "        x = self.leaky_relu(self.layer3(x))\n",
        "        x = self.leaky_relu(self.layer4(x))\n",
        "        x = self.leaky_relu(self.layer5(x))\n",
        "        x = self.layer6(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "8jf8Sq9uv6Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4hvGFMgda1w"
      },
      "outputs": [],
      "source": [
        "class Policy:\n",
        "\n",
        "    def __init__(self, env,epsilon=0.5,epsilon_min=0.013, epsilon_decay=0.9875):\n",
        "        self.number_inputs = env.width * env.height\n",
        "        self.number_actions = env.width\n",
        "        self.qnetwork = QNetwork(self.number_inputs, env.width, env.height, self.number_actions)\n",
        "        self.epsilon = epsilon\n",
        "        self.env = env\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def __call__(self,state,no_epsilon = False):\n",
        "        available_moves = self.env.get_moves()\n",
        "        q_values = self.qnetwork(torch.tensor(state))\n",
        "\n",
        "        if(random.random()<self.epsilon and not no_epsilon):\n",
        "            action = random.choice(available_moves)\n",
        "        else:\n",
        "            best_move = available_moves[0]\n",
        "            best_q_value = q_values[best_move]\n",
        "            for move in available_moves:\n",
        "                if(q_values[move]>best_q_value):\n",
        "                    best_move = move\n",
        "                    best_q_value = q_values[move]\n",
        "            action = best_move\n",
        "\n",
        "        return action\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def reset_epsilon(self, epsilon = 0.5):\n",
        "        self.epsilon = epsilon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def better_step(env,action,player):\n",
        "        observed_space, reward_vector,winner,info = env.step(action)\n",
        "        states = observed_space[player]\n",
        "        empty_states = states[0]\n",
        "        player_states = states[1]\n",
        "        opponent_states = states[2]\n",
        "        done = winner or (len(info['legal_actions'])==0)\n",
        "        trad_state = player_states - opponent_states\n",
        "        return trad_state.flatten(), reward_vector[player],done"
      ],
      "metadata": {
        "id": "KXFneoCBvw8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rz3PbmBda1x"
      },
      "outputs": [],
      "source": [
        "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
        "\n",
        "    def __init__(self, optimizer: torch.optim.Optimizer, lr_decay: float, last_epoch: int = -1, min_lr: float = 1e-6):\n",
        "        self.min_lr = min_lr\n",
        "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
        "\n",
        "    def get_lr(self) -> List[float]:\n",
        "        return [\n",
        "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
        "            for base_lr in self.base_lrs\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive training"
      ],
      "metadata": {
        "id": "cJVALK1rv_eA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-yweG6Zda1y"
      },
      "outputs": [],
      "source": [
        "def train_naive_agent_against(env: gym.Env,\n",
        "                      optimizer: torch.optim.Optimizer,\n",
        "                      first_agent: Policy,\n",
        "                      oponnent:Policy,\n",
        "                      loss_fn: Callable,\n",
        "                      device: torch.device,\n",
        "                      lr_scheduler: _LRScheduler,\n",
        "                      num_episodes: int,\n",
        "                      gamma: float) -> List[float]:\n",
        "    trainee_wins=  0\n",
        "    trainer_wins = 0\n",
        "    episode_reward_list = []\n",
        "\n",
        "    player = 0\n",
        "    for episode_index in tqdm(range(1, num_episodes)):\n",
        "        state = env.reset()\n",
        "        state= state[0][1].flatten()\n",
        "        episode_reward = 0\n",
        "        player = np.random.choice([0,1])\n",
        "\n",
        "        if(player ==1):\n",
        "            opponent_action = oponnent.__call__(state)\n",
        "            state, _= better_step(env, opponent_action,player)\n",
        "\n",
        "        for t in itertools.count():\n",
        "            q_network = first_agent.qnetwork\n",
        "            q_values = q_network(torch.tensor(state))\n",
        "            action = first_agent.__call__(state)\n",
        "            calculated_reward = 0\n",
        "            if(len(env.get_moves()) == 0):\n",
        "                done = True\n",
        "            else:\n",
        "                next_state, done = better_step(env,action,1-player)\n",
        "                if(done):\n",
        "                    calculated_reward = 1\n",
        "\n",
        "                    trainee_wins +=1\n",
        "                else:\n",
        "                    if(len(env.get_moves()) == 0):\n",
        "                        done = True\n",
        "                    else:\n",
        "                        opponent_action = oponnent.__call__(next_state)\n",
        "                        next_state, done = better_step(env, opponent_action,player)\n",
        "                        if(done):\n",
        "                            calculated_reward = -1\n",
        "                            trainer_wins +=1\n",
        "\n",
        "            episode_reward += calculated_reward\n",
        "            next_state_tensor = torch.tensor(next_state)\n",
        "            with torch.no_grad():\n",
        "                target_q_values = calculated_reward + gamma * torch.max(q_network(next_state_tensor))\n",
        "            current_q_value = q_values[action]\n",
        "\n",
        "            loss = loss_fn(current_q_value, target_q_values)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        episode_reward_list.append(episode_reward)\n",
        "        first_agent.decay_epsilon()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(\"End of episode: Trainer winrate : {:.2f}%, Trainee winrate : {:.2f}%, Draw rate : {:.2f}%\".format(100* (trainer_wins/num_episodes), 100* (trainee_wins/num_episodes), 100* ((num_episodes -trainee_wins-trainer_wins)/num_episodes)))\n",
        "    return episode_reward_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replay Buffer"
      ],
      "metadata": {
        "id": "nKaH4R6ywU0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEfOo6tHda1y"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool):\n",
        "        if(self.__len__() >self.capacity):\n",
        "            self.buffer = collections.deque(maxlen=capacity)\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]:\n",
        "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with replay buffer"
      ],
      "metadata": {
        "id": "bPShhclgwc85"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeB9bCn3da1z"
      },
      "outputs": [],
      "source": [
        "def train_dqn1_agent(env: gym.Env,\n",
        "                     first_agent:Policy,\n",
        "                     oponnent:Policy,\n",
        "                     optimizer: torch.optim.Optimizer,\n",
        "                     loss_fn: Callable,\n",
        "                     device: torch.device,\n",
        "                     lr_scheduler: _LRScheduler,\n",
        "                     num_episodes: int,\n",
        "                     gamma: float,\n",
        "                     batch_size: int,\n",
        "                     replay_buffer: ReplayBuffer,\n",
        "                     render:bool) -> List[float]:\n",
        "\n",
        "    episode_reward_list = []\n",
        "    q_network = first_agent.qnetwork\n",
        "    player = 0\n",
        "    for episode_index in range(1, num_episodes):\n",
        "        state= env.reset()\n",
        "        state = state[0][1].flatten()\n",
        "\n",
        "        episode_reward = 0\n",
        "        player = np.random.choice([0,1])\n",
        "        if(player == 1):\n",
        "            opponent_action = oponnent.__call__(state)\n",
        "            state, reward, done = better_step(env, opponent_action,player)\n",
        "        for t in itertools.count():\n",
        "\n",
        "            if(len(env.get_moves())==0):\n",
        "                done = True\n",
        "                reward = 0\n",
        "            else:\n",
        "                action = first_agent.__call__(state)\n",
        "                next_state, done, reward = better_step(env, action, 1-player)\n",
        "                if(not done and len(env.get_moves()) !=0):\n",
        "                    opponent_action = oponnent.__call__(next_state)\n",
        "                    next_state, reward,done = better_step(env, opponent_action, player)\n",
        "                    reward = -1*reward\n",
        "\n",
        "            replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "                states = torch.tensor(states, device=device)\n",
        "                actions = torch.tensor(actions,  device=device)\n",
        "                rewards = torch.tensor(rewards, device=device)\n",
        "                next_states = torch.tensor(next_states,  device=device)\n",
        "                dones = torch.tensor(dones,device=device)\n",
        "                q_network= first_agent.qnetwork\n",
        "                q_values = q_network(states)\n",
        "                next_q_values = q_network(next_states)\n",
        "                target_q_values = rewards + gamma * torch.max(next_q_values, dim=1).values * ~dones\n",
        "\n",
        "                loss = loss_fn(q_values.gather(1, actions.unsqueeze(1)), target_q_values.unsqueeze(1))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "        episode_reward_list.append(episode_reward)\n",
        "        first_agent.decay_epsilon()\n",
        "\n",
        "    return episode_reward_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOFqC1o1da10"
      },
      "source": [
        "<h1> Training with DQN - not Naive function </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train different agents with varying batch sizes to sample from the replay buffer and save them."
      ],
      "metadata": {
        "id": "m0l2TG0xxv02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Connect4-v0')\n",
        "print(dir(env))"
      ],
      "metadata": {
        "id": "uIIcxn7FYo4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Ux1imNda12"
      },
      "outputs": [],
      "source": [
        "number_of_training_done = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size = 1\n",
        "env = gym.make('Connect4-v0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUMBER_OF_TRAININGS = 1000\n",
        "naive_trains_result_list = [[], [], []]\n",
        "\n",
        "first_agent = 1\n",
        "opponent_agent = 2\n",
        "\n",
        "first_agent = Policy(env)\n",
        "opponent_agent = Policy(env)\n",
        "\n",
        "for train_index in tqdm(range(NUMBER_OF_TRAININGS)):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(first_agent.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    first_agent.epsilon = 0.9\n",
        "    episode_reward_list = train_dqn1_agent(env,\n",
        "                                            first_agent,\n",
        "                                            opponent_agent,\n",
        "                                            optimizer,\n",
        "                                            loss_fn,\n",
        "                                            device,\n",
        "                                            lr_scheduler,\n",
        "                                            num_episodes=200, #200 episodes * 1000 trainings = 200k games\n",
        "                                            gamma=0.9,\n",
        "                                            replay_buffer=replay_buffer,\n",
        "                                            batch_size=1,\n",
        "                                            render=False\n",
        "\n",
        "                                            )\n",
        "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    naive_trains_result_list[1].extend(episode_reward_list)\n",
        "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "    if(train_index % 10 == 0):\n",
        "            opponent_agent = 2\n",
        "            opponent_agent = Policy(env)\n",
        "            opponent_agent.qnetwork.load_state_dict(first_agent.qnetwork.state_dict())\n",
        "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
        "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
        "\n",
        "\n",
        "torch.save(first_agent.qnetwork, \"DQN_200k_with_1_Buffer.pth\")\n",
        "\n",
        "number_of_training_done+=1\n",
        "env.close()"
      ],
      "metadata": {
        "id": "mz0CqHqTYq8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size = 10\n",
        "env = gym.make('Connect4-v0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUMBER_OF_TRAININGS = 1000\n",
        "naive_trains_result_list = [[], [], []]\n",
        "\n",
        "first_agent = 1\n",
        "opponent_agent = 2\n",
        "\n",
        "first_agent = Policy(env)\n",
        "opponent_agent = Policy(env)\n",
        "for train_index in tqdm(range(NUMBER_OF_TRAININGS)):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(first_agent.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    first_agent.epsilon = 0.9\n",
        "    episode_reward_list = train_dqn1_agent(env,\n",
        "                                            first_agent,\n",
        "                                            opponent_agent,\n",
        "                                            optimizer,\n",
        "                                            loss_fn,\n",
        "                                            device,\n",
        "                                            lr_scheduler,\n",
        "                                            num_episodes=200,\n",
        "                                            gamma=0.9,\n",
        "                                            replay_buffer=replay_buffer,\n",
        "                                            batch_size=10,\n",
        "                                            render=False\n",
        "\n",
        "                                            )\n",
        "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    naive_trains_result_list[1].extend(episode_reward_list)\n",
        "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "    if(train_index % 10 == 0):\n",
        "            opponent_agent = 2\n",
        "            opponent_agent = Policy(env)\n",
        "            opponent_agent.qnetwork.load_state_dict(first_agent.qnetwork.state_dict())\n",
        "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
        "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
        "\n",
        "torch.save(first_agent.qnetwork, \"DQN_200k_with_10_Buffer.pth\")\n",
        "\n",
        "number_of_training_done+=1\n",
        "env.close()"
      ],
      "metadata": {
        "id": "nPw0umbYYsDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size = 100\n",
        "env = gym.make('Connect4-v0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUMBER_OF_TRAININGS = 1000\n",
        "naive_trains_result_list = [[], [], []]\n",
        "\n",
        "first_agent = 1\n",
        "opponent_agent = 2\n",
        "\n",
        "first_agent = Policy(env)\n",
        "opponent_agent = Policy(env)\n",
        "for train_index in tqdm(range(NUMBER_OF_TRAININGS)):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(first_agent.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    first_agent.epsilon = 0.9\n",
        "    episode_reward_list = train_dqn1_agent(env,\n",
        "                                            first_agent,\n",
        "                                            opponent_agent,\n",
        "                                            optimizer,\n",
        "                                            loss_fn,\n",
        "                                            device,\n",
        "                                            lr_scheduler,\n",
        "                                            num_episodes=200,\n",
        "                                            gamma=0.9,\n",
        "                                            replay_buffer=replay_buffer,\n",
        "                                            batch_size=100,\n",
        "                                            render=False\n",
        "\n",
        "                                            )\n",
        "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    naive_trains_result_list[1].extend(episode_reward_list)\n",
        "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "    if(train_index % 10 == 0):\n",
        "            opponent_agent = 2\n",
        "            opponent_agent = Policy(env)\n",
        "            opponent_agent.qnetwork.load_state_dict(first_agent.qnetwork.state_dict()) #Load opponent as trained one\n",
        "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
        "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
        "\n",
        "torch.save(first_agent.qnetwork, \"DQN_200k_with_100_Buffer.pth\")\n",
        "\n",
        "number_of_training_done+=1\n",
        "env.close()"
      ],
      "metadata": {
        "id": "U9jIrrauYtnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size = 1000\n",
        "env = gym.make('Connect4-v0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUMBER_OF_TRAININGS = 1000\n",
        "naive_trains_result_list = [[], [], []]\n",
        "\n",
        "first_agent = 1\n",
        "opponent_agent = 2\n",
        "\n",
        "first_agent = Policy(env)\n",
        "opponent_agent = Policy(env)\n",
        "for train_index in tqdm(range(NUMBER_OF_TRAININGS)):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(first_agent.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    first_agent.epsilon = 0.9\n",
        "    episode_reward_list = train_dqn1_agent(env,\n",
        "                                            first_agent,\n",
        "                                            opponent_agent,\n",
        "                                            optimizer,\n",
        "                                            loss_fn,\n",
        "                                            device,\n",
        "                                            lr_scheduler,\n",
        "                                            num_episodes=200,\n",
        "                                            gamma=0.9,\n",
        "                                            replay_buffer=replay_buffer,\n",
        "                                            batch_size=1000,\n",
        "                                            render=False\n",
        "\n",
        "                                            )\n",
        "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    naive_trains_result_list[1].extend(episode_reward_list)\n",
        "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "    if(train_index % 10 == 0):\n",
        "            opponent_agent = 2\n",
        "            opponent_agent = Policy(env)\n",
        "            opponent_agent.qnetwork.load_state_dict(first_agent.qnetwork.state_dict())\n",
        "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
        "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
        "\n",
        "torch.save(first_agent.qnetwork, \"DQN_200k_with_1k_Buffer.pth\")\n",
        "\n",
        "number_of_training_done+=1\n",
        "env.close()"
      ],
      "metadata": {
        "id": "cxSu1QXYYuiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size = 5000\n",
        "env = gym.make('Connect4-v0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUMBER_OF_TRAININGS = 1000\n",
        "naive_trains_result_list = [[], [], []]\n",
        "\n",
        "first_agent = 1\n",
        "opponent_agent = 2\n",
        "\n",
        "first_agent = Policy(env)\n",
        "opponent_agent = Policy(env)\n",
        "for train_index in tqdm(range(NUMBER_OF_TRAININGS)):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(first_agent.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    first_agent.epsilon = 0.9\n",
        "    episode_reward_list = train_dqn1_agent(env,\n",
        "                                            first_agent,\n",
        "                                            opponent_agent,\n",
        "                                            optimizer,\n",
        "                                            loss_fn,\n",
        "                                            device,\n",
        "                                            lr_scheduler,\n",
        "                                            num_episodes=200,\n",
        "                                            gamma=0.9,\n",
        "                                            replay_buffer=replay_buffer,\n",
        "                                            batch_size=5000,\n",
        "                                            render=False\n",
        "\n",
        "                                            )\n",
        "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    naive_trains_result_list[1].extend(episode_reward_list)\n",
        "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "    if(train_index % 10 == 0):\n",
        "            opponent_agent = 2\n",
        "            opponent_agent = Policy(env)\n",
        "            opponent_agent.qnetwork.load_state_dict(first_agent.qnetwork.state_dict())\n",
        "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
        "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
        "\n",
        "torch.save(first_agent.qnetwork, \"DQN_200k_with_5k_Buffer.pth\")\n",
        "\n",
        "number_of_training_done+=1\n",
        "env.close()"
      ],
      "metadata": {
        "id": "vmV6S6ZMYwTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch size = 9000\n",
        "env = gym.make('Connect4-v0')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "NUMBER_OF_TRAININGS = 1000\n",
        "naive_trains_result_list = [[], [], []]\n",
        "\n",
        "first_agent = 1\n",
        "opponent_agent = 2\n",
        "\n",
        "first_agent = Policy(env)\n",
        "opponent_agent = Policy(env)\n",
        "for train_index in tqdm(range(NUMBER_OF_TRAININGS)):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(first_agent.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    replay_buffer = ReplayBuffer(10000)\n",
        "    first_agent.epsilon = 0.9\n",
        "    episode_reward_list = train_dqn1_agent(env,\n",
        "                                            first_agent,\n",
        "                                            opponent_agent,\n",
        "                                            optimizer,\n",
        "                                            loss_fn,\n",
        "                                            device,\n",
        "                                            lr_scheduler,\n",
        "                                            num_episodes=200,\n",
        "                                            gamma=0.9,\n",
        "                                            replay_buffer=replay_buffer,\n",
        "                                            batch_size=9000,\n",
        "                                            render=False\n",
        "\n",
        "                                            )\n",
        "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    naive_trains_result_list[1].extend(episode_reward_list)\n",
        "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "    if(train_index % 10 == 0):\n",
        "            opponent_agent = 2\n",
        "            opponent_agent = Policy(env)\n",
        "            opponent_agent.qnetwork.load_state_dict(first_agent.qnetwork.state_dict())\n",
        "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
        "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
        "\n",
        "torch.save(first_agent.qnetwork, \"DQN_200k_with_9k_Buffer.pth\")\n",
        "\n",
        "number_of_training_done+=1\n",
        "env.close()"
      ],
      "metadata": {
        "id": "mce3bCikYyON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QItHJ2Eada14"
      },
      "source": [
        "<h1> Testing two Agent in 1vs1 </h1>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def play_a_game(env, first_agent, oponnent, no_epsilon_1 = False, no_epsilon_2 = False,render=False):\n",
        "        state = env.reset()\n",
        "        state= state[0][1].flatten()\n",
        "        episode_reward = 0\n",
        "        player = np.random.choice([0,1])\n",
        "\n",
        "        if(player ==1):\n",
        "            opponent_action = oponnent.__call__(state, no_epsilon_2)\n",
        "            state, _,_= better_step(env, opponent_action,player)\n",
        "\n",
        "            if(render): env.render()\n",
        "\n",
        "        for t in itertools.count():\n",
        "            q_network = first_agent.qnetwork\n",
        "            q_values = q_network(torch.tensor(state))\n",
        "\n",
        "            action = first_agent.__call__(state, no_epsilon=no_epsilon_1)\n",
        "            calculated_reward = 0\n",
        "            if(len(env.get_moves()) == 0):\n",
        "                done = True\n",
        "\n",
        "            else:\n",
        "\n",
        "                next_state,_, done = better_step(env,action,1-player)#\n",
        "                if(render): env.render()\n",
        "                if(done):\n",
        "                    calculated_reward = 1\n",
        "\n",
        "                else:\n",
        "                    if(len(env.get_moves()) == 0):\n",
        "                        done = True\n",
        "\n",
        "                    else:\n",
        "\n",
        "                        opponent_action = oponnent.__call__(next_state,no_epsilon=no_epsilon_2)\n",
        "                        next_state, _,done = better_step(env, opponent_action,player)\n",
        "\n",
        "                        if(render): env.render()\n",
        "                        if(done):\n",
        "                            calculated_reward = -1\n",
        "\n",
        "            episode_reward += calculated_reward\n",
        "            next_state_tensor = torch.tensor(next_state)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "        return calculated_reward"
      ],
      "metadata": {
        "id": "DZnlfdjYy8xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_a_game_with_train(env, first_agent, oponnent,optimizer, loss, gamma, lr_scheduler,render=False,replay_buffer = ReplayBuffer(10000),batch_size=128):\n",
        "        state = env.reset()\n",
        "        state= state[0][1].flatten()\n",
        "        episode_reward = 0\n",
        "        player = np.random.choice([0,1])\n",
        "\n",
        "\n",
        "        if(player ==1):\n",
        "            opponent_action = oponnent.__call__(state)\n",
        "            state, _,_= better_step(env, opponent_action,player)\n",
        "\n",
        "\n",
        "        for t in itertools.count():\n",
        "            q_network = first_agent.qnetwork\n",
        "            q_values = q_network(torch.tensor(state))\n",
        "            action = first_agent.__call__(state)\n",
        "            calculated_reward = 0\n",
        "            if(len(env.get_moves()) == 0):\n",
        "                done = True\n",
        "            else:\n",
        "                next_state, _,done = better_step(env,action,1-player)\n",
        "                if(done):\n",
        "                    calculated_reward = 1\n",
        "                else:\n",
        "                    if(len(env.get_moves()) == 0):\n",
        "                        done = True\n",
        "                    else:\n",
        "                        opponent_action = oponnent.__call__(next_state)\n",
        "                        next_state, _,done = better_step(env, opponent_action,player)\n",
        "                        if(done):\n",
        "                            calculated_reward = -1\n",
        "            episode_reward += calculated_reward\n",
        "            next_state_tensor = torch.tensor(next_state)\n",
        "\n",
        "            next_state_tensor = torch.tensor(next_state)\n",
        "\n",
        "            replay_buffer.add(state, action, calculated_reward, next_state, done)\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "                states = torch.tensor(states, device=device)\n",
        "                actions = torch.tensor(actions,  device=device)\n",
        "                rewards = torch.tensor(rewards, device=device)\n",
        "                next_states = torch.tensor(next_states,  device=device)\n",
        "                dones = torch.tensor(dones,device=device)\n",
        "                q_network= first_agent.qnetwork\n",
        "                q_values = q_network(states)\n",
        "                next_q_values = q_network(next_states)\n",
        "                target_q_values = rewards + gamma * torch.max(next_q_values, dim=1).values * ~dones\n",
        "\n",
        "                loss = loss_fn(q_values.gather(1, actions.unsqueeze(1)), target_q_values.unsqueeze(1))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                lr_scheduler.step()\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        first_agent.decay_epsilon()\n",
        "        lr_scheduler.step()\n",
        "        return calculated_reward\n"
      ],
      "metadata": {
        "id": "Ti8IqR4WzDkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
        "lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "NUMBER_OF_GAMES = 10000\n",
        "\n",
        "no_epsilon_1 = False #True if games are played without epsilon policy\n",
        "no_epsilon_2 = False"
      ],
      "metadata": {
        "id": "HIUsAtnD4FXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_1_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=1)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "zC-k0lGl2Svl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 1, no adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tojtSUUZzo6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 1, with adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L-AHa82YzqNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_10_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=10)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "B6FjKA5c2rFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 10, no adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D6EOvhR22wAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 10, with adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zkKwcq4v2zgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_100_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=100)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "v_HoJ5N625lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 100, no adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EsbSeMlI29RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 100, with adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Am6ledu3AeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_1k_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=1000)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "7F-N35RE2VMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 1000, no adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sZc4hVncz7i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 1000, with adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p6_OsJ2x0AGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_5k_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=5000)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "JLLsPBwg2WqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 5000, no adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DfqcTUUj0RKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 5000, with adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yVGqfNLb0TQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_9k_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=9000)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "XPQ2Ic4i2YgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 9000, no adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LOMQcy_D0eZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 9000, with adaptation, with $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zsCbUqHn0fI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Impact of epsilon-greedy policy </h1>"
      ],
      "metadata": {
        "id": "wLCka2B44bD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_epsilon_1 = True #True if games are played without epsilon policy\n",
        "no_epsilon_2 = True"
      ],
      "metadata": {
        "id": "hEZhb97E4ZvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Agent_1 = 1\n",
        "Agent_2 = 1\n",
        "\n",
        "Agent_1 = Policy(env)\n",
        "Agent_1.qnetwork = torch.load(\"DQN_200k_with_9k_Buffer.pth\")\n",
        "Agent_2 = Policy(env)\n",
        "\n",
        "optimizer = torch.optim.AdamW(Agent_1.qnetwork.parameters(), lr=0.004, amsgrad=True)\n",
        "\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr0 = 1\n",
        "wrs0 = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game(env, Agent_1, Agent_2, no_epsilon_1, no_epsilon_2)\n",
        "\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr0 = (wr0*i + 1)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr0 = (wr0*i)/(i+1)\n",
        "        wrs0.append(wr0)\n",
        "    else:\n",
        "        draws+=1\n",
        "agent_wins = 0\n",
        "opponent_wins = 0\n",
        "draws = 0\n",
        "wr = 1\n",
        "wrs = []\n",
        "for i in tqdm(range(NUMBER_OF_GAMES)):\n",
        "    res = play_a_game_with_train(env, Agent_1, Agent_2,optimizer, loss_fn,0.5,lr_scheduler,batch_size=9000)\n",
        "    if(res == 1):\n",
        "        agent_wins+=1\n",
        "        wr = (wr*i + 1)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    elif(res ==-1):\n",
        "        opponent_wins +=1\n",
        "        wr = (wr*i)/(i+1)\n",
        "        wrs.append(wr)\n",
        "    else:\n",
        "        draws+=1"
      ],
      "metadata": {
        "id": "jJrwAgW64aPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs0 = wrs0[600:]\n",
        "plt.title('Batch size = 9000, no adaptation, without $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs0), len(wrs0)),wrs0,color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OPMrH3vd4kjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrs = wrs[600:]\n",
        "plt.title('Batch size = 9000, no adaptation, without $\\epsilon$-greedy policy')\n",
        "plt.xlabel('Number of Games')\n",
        "plt.ylabel('Winrate')\n",
        "plt.plot(np.linspace(1,len(wrs), len(wrs)),wrs,color='g')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRZ8f3CI4mlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EroXIgxNda2B"
      },
      "source": [
        "<h1> Checking Inside an Agent </h1>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_agent = Agent_1\n",
        "for name, param in first_agent.qnetwork.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        print(f'Layer: {name}, Coefficients:')\n",
        "        print(param.data)\n",
        "        print()"
      ],
      "metadata": {
        "id": "tJe2GK5c2ffc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opponent = Agent_2\n",
        "\n",
        "for name, param in opponent.qnetwork.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        print(f'Layer: {name}, Coefficients:')\n",
        "        print(param.data)\n",
        "        print()"
      ],
      "metadata": {
        "id": "3DVz9JnU2gWp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "RLEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}